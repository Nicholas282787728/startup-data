{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s work on the following:\n",
    "1. Access the CB API (with my login info), which is inside Dropbox. Download the list of companies founded in 2016 and with category ‘Artificial Intelligence’. That should be 500sh companies, we start with a small sample.\n",
    "2. For each of those companies, download all the NEWS that Crunchbase has. Try to put those news into an easy format: maybe a TXT file per news, and all TXT files of a company into one folder. And remove all non-relevant components (like Ads or other text). This will make it easier if somebody in the team will have to do some reading.\n",
    "3. Once this looks good, we’ll do some NLP on the text. But let’s get the \"raw data\" first.\n",
    "4. Please upload all material on Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import sys\n",
    "import codecs\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from readability.readability import Document # https://github.com/buriy/python-readability. Tried Goose, Newspaper (python libraries on Github). Bad results.\n",
    "from http.cookiejar import CookieJar # \n",
    "\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import ast\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import shutil\n",
    "\n",
    "import articleDateExtractor\n",
    "from newspaper import Article\n",
    "\n",
    "import pycrunchbase\n",
    "cb = pycrunchbase.CrunchBase('662e263576fe3e4ea5991edfbcfb9883')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns=[\"permalink\"])\n",
    "\n",
    "# url = 'https://api.crunchbase.com/v3.1/organizations/?user_key=662e263576fe3e4ea5991edfbcfb9883'\n",
    "# # location given here \n",
    "# location = \"philadephia\"\n",
    "\n",
    "# # defining a params dict for the parameters to be sent to the API \n",
    "# PARAMS = {'address':location} \n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         response = requests.get(url, PARAMS)\n",
    "#         json = response.json()\n",
    "#         for c in json['data']['items']:\n",
    "#             permalink=c['properties']['permalink']\n",
    "#             if(permalink!=None):\n",
    "#                 categories_json = requests.get(\"https://api.crunchbase.com/v3.1/organizations/\" \n",
    "#                                 + permalink+\"/categories?user_key=662e263576fe3e4ea5991edfbcfb9883\", PARAMS).json()\n",
    "#                 for cat in categories_json['data']['items']:\n",
    "#                     if(cat[\"properties\"][\"category_groups\"]!=None):\n",
    "#                         if(\"Artificial Intelligence\" in cat[\"properties\"][\"category_groups\"]):\n",
    "#                             print(permalink)\n",
    "#                             df = df.append({'permalink' : permalink} , ignore_index=True)\n",
    "#         url = json['data']['paging']['next_page_url'] + \"&user_key=662e263576fe3e4ea5991edfbcfb9883\"\n",
    "#         print(url)\n",
    "#     except:\n",
    "#         print(\"error: \" + url)\n",
    "        \n",
    "# df\n",
    "\n",
    "df = pd.read_csv('csv_export/organizations.csv')\n",
    "print(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df = pd.DataFrame(columns = df.columns)\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        if(\"Artificial Intelligence\" in df['category_group_list'][i] and '2016' in df['founded_on'][i]):\n",
    "            ai_df = ai_df.append(df.loc[i])\n",
    "    except:\n",
    "        pass\n",
    "ai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df= ai_df.reset_index(drop=True)\n",
    "ai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# location given here \n",
    "location = \"philadephia\"\n",
    "\n",
    "# defining a params dict for the parameters to be sent to the API \n",
    "PARAMS = {'address':location} \n",
    "for i in range(len(ai_df)):\n",
    "    permalink = ai_df['permalink'][i].split(\"/\")[2]\n",
    "    newpath = '/Users/ksun/downloads/startup-data/ai-organizations/' + permalink\n",
    "    if not os.path.exists(newpath):\n",
    "        print(\"create new folder\", newpath)\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "        url = 'https://api.crunchbase.com/v3.1/organizations/' + permalink+'?relationships=news&user_key=662e263576fe3e4ea5991edfbcfb9883'\n",
    "        response_json = requests.get(url, PARAMS).json()\n",
    "\n",
    "        for n in response_json['data']['relationships']['news']['items']:\n",
    "            try:\n",
    "                uri=n['properties']['url']\n",
    "                opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor)\n",
    "                opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.77 Safari/535.7')]\n",
    "                html =  opener.open(uri).read().decode('utf-8')\n",
    "                readable_article = Document(html).summary()\n",
    "                soup = BeautifulSoup(readable_article, \"lxml\")\n",
    "                text = soup.get_text()\n",
    "\n",
    "                name_of_file = uri.replace(\".\", \"-\").replace(\":\", \"-\").replace(\"/\", \"-\")\n",
    "                path = newpath + \"/\"\n",
    "\n",
    "                completeName = os.path.join(path, name_of_file+\".txt\")  \n",
    "\n",
    "\n",
    "                file1 = open(completeName, \"w\")\n",
    "\n",
    "                file1.write(text)\n",
    "\n",
    "                file1.close()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if len(os.listdir(newpath)) == 0:\n",
    "        print(newpath)\n",
    "    \n",
    "        url = 'https://api.crunchbase.com/v3.1/organizations/' + permalink+'?relationships=news&user_key=662e263576fe3e4ea5991edfbcfb9883'\n",
    "        response_json = requests.get(url, PARAMS).json()\n",
    "        print(url)\n",
    "\n",
    "        for n in response_json['data']['relationships']['news']['items']:\n",
    "            try:\n",
    "                uri=n['properties']['url']\n",
    "                print(uri)\n",
    "                opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor)\n",
    "                opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.77 Safari/535.7')]\n",
    "                html =  opener.open(uri).read().decode('utf-8')\n",
    "                readable_article = Document(html).summary()\n",
    "                soup = BeautifulSoup(readable_article, \"lxml\")\n",
    "                text = soup.get_text()\n",
    "\n",
    "                name_of_file = uri.replace(\".\", \"-\").replace(\":\", \"-\").replace(\"/\", \"-\")\n",
    "                path = newpath + \"/\"\n",
    "\n",
    "                completeName = os.path.join(path, name_of_file+\".txt\")  \n",
    "\n",
    "\n",
    "                file1 = open(completeName, \"w\")\n",
    "\n",
    "                file1.write(text)\n",
    "                if(len(text) != 0):\n",
    "                    print(\"write complete:\", newpath)\n",
    "\n",
    "                file1.close()\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi Kevin\n",
    "Thanks again for coming in! And great job – this is an excellent first step.\n",
    "Here are the next steps:\n",
    "1. I went through the folders – most look good, but it’s possible there’s some missing material. Spend a little more time to perfect the downloading  code. See if you can fix whichever errors you find through the process. It might be best to make this code as smooth as possible now, before we scale to working with lots of companies.\n",
    "2. I am assuming the scraping process is OK, but it’s probably helpful to take one more look at that too (i.e. for example, those few lines missing in the TXT we looked at together may happen elsewhere too, no huge deal but worth looking into it).\n",
    "3. Once you’ve finalized the downloading process, let’s start with a very simple exercise on the TXT files. For each TXT of each company, write a code that counts the number of times the following keywords are mentioned: PROTOTYPE, TEST, EXPERIMENT, MINIMUM VIABLE PRODUCT, BETA, ALPHA. This is just a start, we’ll do more of this, but let’s see how this goes. You can then input this data into an Excel file (one row per company).\n",
    " \n",
    "If possible, please write your Python code clearly and upload it. So I can try to take a look.\n",
    " \n",
    "Please remember to submit your timesheet by Thursday night each week. Let’s try to meet week after Spring break, I am very flexible. Of course, please reach out if you have any questions at any point.\n",
    "Thanks so much!\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/ksun/downloads/startup-data/ai-organizations/'\n",
    "\n",
    "ai_df['articles'] = np.empty((len(ai_df), 0)).tolist()\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if '.DS_Store' not in file:\n",
    "#             if(len(ai_df[ai_df['permalink'].astype(str).str.contains('/organization/' + subdir.split(\"/\")[-1])])==0):\n",
    "#                 print('/organization/' + subdir.split(\"/\")[-1])\n",
    "#                 shutil.rmtree(subdir)\n",
    "#                 break\n",
    "            row_index = ai_df.loc[ai_df['permalink'] == '/organization/' + subdir.split(\"/\")[-1]].index.values.astype(int)[0]\n",
    "            file = open(subdir +  \"/\" + file, \"r\")\n",
    "            ai_df['articles'][row_index].append(file.read())\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_df.to_csv('csv_export/ai_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_df = pd.read_csv('csv_export/ai_df_old.csv', index_col=0)\n",
    "# ai_df\n",
    "# ai_df.drop(['Unnamed: 0'], axis=1)\n",
    "# ai_df.to_csv('csv_export/ai_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather good data from twitter (make sure to scrape over several months, stop at December 2018\n",
    "2. topic clusters, sentiment, OVER TIME (month level) for a specific company\n",
    "3. Take one old company like Google and track over time\n",
    "__________________________\n",
    "Let’s continue to work on the 2016 sample of AI companies for now, just to keep it simple.\n",
    "1. Let’s add news from TWITTER and GOOGLE NEWS. You decide how to better get the right material from these two sources.\n",
    "2. Once we have all these sources in place, we can start by doing a simple sentiment analysis. Each company should have a time series of sentiment measures (i.e. number of negative news): so each company should be have observations between first month of life and Dec 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# location given here \n",
    "location = \"philadephia\"\n",
    "\n",
    "ai_df['articles'] = np.empty((len(ai_df), 0)).tolist()\n",
    "\n",
    "# defining a params dict for the parameters to be sent to the API \n",
    "HEADERS = {'accept':\"application/json\"} \n",
    "for i in range(len(ai_df)):\n",
    "    permalink = ai_df['permalink'][i].split(\"/\")[2]\n",
    "    \n",
    "    url = 'https://api.crunchbase.com/v3.1/organizations/' + permalink+'?relationships=news&user_key=662e263576fe3e4ea5991edfbcfb9883'\n",
    "    try:\n",
    "        response_json = requests.get(url, headers=HEADERS, params={\"$filter\":\"Path eq '/xxxxxx/'\"}).json()\n",
    "        print(url)\n",
    "\n",
    "        for n in response_json['data']['relationships']['news']['items']:\n",
    "\n",
    "            try:\n",
    "                uri=n['properties']['url']\n",
    "                opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor)\n",
    "                opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.77 Safari/535.7')]\n",
    "                html =  opener.open(uri).read().decode('utf-8')\n",
    "                readable_article = Document(html).summary()\n",
    "                soup = BeautifulSoup(readable_article, \"lxml\")\n",
    "                text = soup.get_text()\n",
    "\n",
    "    #             article = Article(uri)\n",
    "    #             article.parse()\n",
    "                date = n['properties']['posted_on']\n",
    "    #             date = articleDateExtractor.extractArticlePublishedDate(uri)\n",
    "\n",
    "\n",
    "                ai_df['articles'][i].append([uri, date, text])\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"error:\", permalink)\n",
    "    except:\n",
    "        print(\"error\", url)\n",
    "        #ugly-research\n",
    "        #recomomenderx, geopipe-inc, cloudzero, ekkono-solutions, trials-ai, quali-fit, vidrovr, ar-cadia, ns8-inc, vapispace, satisfy, gravyty, ignite-biosciences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(ai_df)):\n",
    "    if(len(ai_df['articles'][i])>10):\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for coming in, Kevin!\n",
    " \n",
    "I think we’re doing great progress.\n",
    " \n",
    "Here's the next steps of this  task:\n",
    "1. Let’s focus on just 1 company and just Twitter. We should try to get all tweets from that company and about that company (from the very beginning to Dec 31st 2018).\n",
    "2. Let’s allocate the tweets on each month from the month in which the Twitter account opens.\n",
    "3. Let’s run sentiment analysis and graph sentiment over all months. You could probably do a separate graph for tweets from the company and tweets about the company. The graph should be a time series, where axis X is date and Y is sentiment. It should basically look like a curve that goes on over time.\n",
    "4. To do this, pick a good company: one that has enough tweets.\n",
    "5. If the process is not to heavy, let’s scale it to 100 companies. Just pick 100 companies from your sample.\n",
    " \n",
    "You can take a look at how much it’d be to move the process to Amazon Web Services. Let me know how it looks and we can then decide.\n",
    " \n",
    "If possible, please send me an update sometimes next week and then we can probably meet the week of Apr 8th. (No need to meet next week!)\n",
    " \n",
    "Thanks again!\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df['twitter_url'][142].split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  python Exporter.py --username \"viralheat\" --since 2016-01-01 --until 2018-12-31\n",
    "# python Exporter.py --username \"kernelco\" --since 2016-01-01 --until 2018-12-31 --maxtweets 100\n",
    "# python main.py --username \"kernelco\" --since 2016-01-01 --until 2018-12-31 --max-tweets 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df['num_prototype'] = 0\n",
    "ai_df['num_test'] = 0\n",
    "ai_df['num_experiment'] = 0\n",
    "ai_df['num_minimum_viable_product'] = 0\n",
    "ai_df['num_beta'] = 0\n",
    "ai_df['num_alpha'] = 0\n",
    "for i in range(len(ai_df)):\n",
    "    wordlist = []\n",
    "    for a in ai_df['articles'][i]:\n",
    "        wordlist.extend(list(chain(*[word_tokenize(s) for s in sent_tokenize(a)])))\n",
    "    counter = Counter(wordlist)\n",
    "    ai_df.loc[i, 'num_prototype'] = counter['prototype']\n",
    "    ai_df.loc[i, 'num_test'] = counter['test']\n",
    "    ai_df.loc[i, 'num_experiment'] = counter['experiment']\n",
    "    ai_df.loc[i, 'num_minimum_viable_product'] = counter['viable']\n",
    "    ai_df.loc[i, 'num_beta'] = counter['beta']\n",
    "    ai_df.loc[i, 'num_alpha'] = counter['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/ksun/downloads/startup-data/GetOldTweets-python'\n",
    "\n",
    "ai_df['tweets'] = np.empty((len(ai_df), 0)).tolist()\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if '.DS_Store' not in file and file in ['mobalytics-2.csv', 'swisscognitive.csv', 'prisma-5.csv']:\n",
    "#             if(len(ai_df[ai_df['permalink'].astype(str).str.contains('/organization/' + subdir.split(\"/\")[-1])])==0):\n",
    "#                 print('/organization/' + subdir.split(\"/\")[-1])\n",
    "#                 shutil.rmtree(subdir)\n",
    "#                 break\n",
    "            row_index = ai_df.loc[ai_df['permalink'] == '/organization/' + file[0:-4]].index.values.astype(int)[0]\n",
    "#             file = open(subdir +  \"/\" + file, \"r\")\n",
    "            print(subdir +  \"/\" + file)\n",
    "            temp_tweet_df = pd.read_csv(subdir +  \"/\" + file, error_bad_lines=False, sep=';')\n",
    "            for j in range(len(temp_tweet_df)):\n",
    "                try:\n",
    "                    ai_df['tweets'][row_index].append([str(temp_tweet_df['username'][j]),\n",
    "                                                      temp_tweet_df['date'][j],\n",
    "                                                      temp_tweet_df['retweets'][j],\n",
    "                                                      temp_tweet_df['favorites'][j],\n",
    "                                                      str(temp_tweet_df['text'][j])])\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "# import shutil\n",
    "# rootdir = '/Users/ksun/downloads/startup-data/Twitter-Get-Old-Tweets-Scraper/tweets'\n",
    "\n",
    "# ai_df['tweets'] = np.empty((len(ai_df), 0)).tolist()\n",
    "\n",
    "# for subdir, dirs, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         if '.DS_Store' not in file:\n",
    "# #             if(len(ai_df[ai_df['permalink'].astype(str).str.contains('/organization/' + subdir.split(\"/\")[-1])])==0):\n",
    "# #                 print('/organization/' + subdir.split(\"/\")[-1])\n",
    "# #                 shutil.rmtree(subdir)\n",
    "# #                 break\n",
    "#             row_index = ai_df.loc[ai_df['permalink'] == '/organization/' + file.split(\"/\")[-1][0:-4]].index.values.astype(int)[0]\n",
    "# #             file = open(subdir +  \"/\" + file, \"r\")\n",
    "#             print(subdir +  \"/\" + file)\n",
    "#             temp_tweet_df = pd.read_csv(subdir +  \"/\" + file, error_bad_lines=False)\n",
    "#             for j in range(len(temp_tweet_df)):\n",
    "#                 try:\n",
    "#                     ai_df['tweets'][row_index].append([str(temp_tweet_df['username'][j]),\n",
    "#                                                       str(temp_tweet_df['user_handle'][j]),\n",
    "#                                                       temp_tweet_df['date'][j],\n",
    "#                                                       temp_tweet_df['retweets'][j],\n",
    "#                                                       temp_tweet_df['favorites'][j],\n",
    "#                                                       str(temp_tweet_df['text'][j])])\n",
    "#                 except:\n",
    "#                     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_df.to_csv('csv_export/ai_df.csv', index=False)\n",
    "ai_df = pd.read_csv('csv_export/ai_df.csv')\n",
    "ai_df\n",
    "# ai_df.drop(['Unnamed: 0'], axis=1)\n",
    "# ai_df.to_csv('csv_export/ai_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweet(tweet): # preprocess tweets\n",
    "    tweet = re.sub(r'\\d+', '', str(tweet)) # remove numbers\n",
    "    tweet = tweet.lower() # convert to lower case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # convert www.* or https?://* to URL\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet) # convert @username to AT_USER\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet) # remove additional white spaces\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # replace #word (hashtags) with word\n",
    "    tweet = tweet.strip('\\'\"') # trim\n",
    "    return tweet    \n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "preprocessTweet(\"@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopWordList(stopWordListFileName): # get stopword list\n",
    "    sw = [] # create list of stopwords\n",
    "    \n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "    for w in nltk_stopwords:\n",
    "        sw.append(w)\n",
    "    sw.append('AT_USER') # special stopwords from preprocessTweet function\n",
    "    sw.append('URL')\n",
    "    \n",
    "    fp = open(stopWordListFileName, 'r') # load in any more custom stopwords from file\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        sw.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return sw\n",
    "\n",
    "\n",
    "stopword_list = getStopWordList('stopwords.txt')\n",
    "print('Stopword List: ', stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureVector(tweet): \n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = replaceTwoOrMore(w) # replace two or more with two occurrences\n",
    "        w = w.strip('\\'\"?,.') # strip punctuation\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w) # check if the word starts with an alphabet\n",
    "        if(w in stopword_list or val is None): # ignore if it is a stop word\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    " \n",
    "getFeatureVector(preprocessTweet(\"@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_train = pd.read_csv(\"Twitter-Get-Old-Tweets-Scraper/training.1600000.processed.noemoticon.csv\", \n",
    "                          header=None, encoding=\"ISO-8859-1\") #latin1 encoding\n",
    "airline_train.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "# airline_train = airline_train.sample(frac=0.05, replace=True, random_state=1)\n",
    "# airline_train = airline_train.reset_index(drop=True)\n",
    "airline_tweets = []\n",
    "airline_featurelist = []\n",
    "for i in range(len(airline_train)):\n",
    "    sentiment = airline_train['sentiment'][i]\n",
    "    tweet = airline_train['text'][i]\n",
    "    preprocessedTweet = preprocessTweet(tweet)\n",
    "    featureVector = getFeatureVector(preprocessedTweet)\n",
    "    airline_featurelist.extend(featureVector)\n",
    "    airline_tweets.append((featureVector, sentiment))\n",
    "        \n",
    "def extract_features(tweet): # Determine if tweet contains a feature word\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in airline_featurelist:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "airline_featurelist = list(set(airline_featurelist)) # remove airline_featurelist duplicates\n",
    "airline_featurelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_training_set = nltk.classify.util.apply_features(extract_features, airline_tweets)\n",
    "airline_NBClassifier = nltk.NaiveBayesClassifier.train(airline_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# joblib.dump(airline_NBClassifier, 'csv_export/NBClassifier.pkl') \n",
    "# NBClassifier = joblib.load('csv_export/NBClassifier.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis\n",
    "model = load_model('twitter-sentiment-model/model.h5')\n",
    "tokenizer = joblib.load('twitter-sentiment-model/tokenizer.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERAS\n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.5, 0.5)\n",
    "\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = NEUTRAL\n",
    "        if score < SENTIMENT_THRESHOLDS[0]:\n",
    "            label = NEGATIVE\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = POSITIVE\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return NEGATIVE if score < 0.5 else POSITIVE\n",
    "\n",
    "def predict(text, include_neutral=True):\n",
    "    start_at = time.time()\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "    # Predict\n",
    "    score = model.predict([x_test])[0]\n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "\n",
    "    return {\"label\": label, \"score\": float(score),\n",
    "       \"elapsed_time\": time.time()-start_at}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_df.loc[ai_df['permalink'] == '/organization/' + 'prisma-5'].index.values.astype(int)[0])\n",
    "print(ai_df.loc[ai_df['permalink'] == '/organization/' + 'swisscognitive'].index.values.astype(int)[0])\n",
    "print(ai_df.loc[ai_df['permalink'] == '/organization/' + 'mobalytics-2'].index.values.astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.literal_eval(str(ai_df['tweets'][814]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(ai_df)):\n",
    "#     tweets = ast.literal_eval(ai_df['tweets'][1904])\n",
    "#     for t in tweets:\n",
    "#         t.append(NBClassifier.classify(extract_features(getFeatureVector(preprocessTweet(t[5])))))\n",
    "#     ai_df.loc[i, 'tweets'] = str(tweets)\n",
    "\n",
    "tweets = ast.literal_eval(str(ai_df['tweets'][67]))\n",
    "for t in tweets:\n",
    "#     try:\n",
    "#         del t[6]\n",
    "#     except:\n",
    "#         pass\n",
    "    s = predict(preprocessTweet(t[4]))['label']\n",
    "    t.append(s)\n",
    "ai_df.loc[67, 'tweets'] = str(tweets)\n",
    "tweets = ast.literal_eval(str(ai_df['tweets'][1893]))\n",
    "for t in tweets:\n",
    "#     try:\n",
    "#         del t[6]\n",
    "#     except:\n",
    "#         pass\n",
    "    s = predict(preprocessTweet(t[4]))['label']\n",
    "    t.append(s)\n",
    "ai_df.loc[1893, 'tweets'] = str(tweets)\n",
    "tweets = ast.literal_eval(str(ai_df['tweets'][814]))\n",
    "for t in tweets:\n",
    "#     try:\n",
    "#         del t[6]\n",
    "#     except:\n",
    "#         pass\n",
    "    s = predict(preprocessTweet(t[4]))['label']\n",
    "    t.append(s)\n",
    "ai_df.loc[814, 'tweets'] = str(tweets)\n",
    "ai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df_vis = pd.DataFrame(columns=['permalink', 'day', 'negative_count', 'positive_count'])\n",
    "\n",
    "checked_days = [] # keep track of which days we have already added to airline_data\n",
    "for i in range(len(ai_df)):\n",
    "    if i==814:\n",
    "        tweets = ast.literal_eval(ai_df['tweets'][i])\n",
    "        for j in range(len(tweets)):\n",
    "            cur_date = str(datetime.datetime.strptime(tweets[j][1].split(\" \")[0], '%Y-%m-%d').strftime('%Y-%m'))\n",
    "            if cur_date not in checked_days:\n",
    "                print(tweets[j][1])\n",
    "                checked_days.append(cur_date)\n",
    "                try:\n",
    "                    temp = list(filter(lambda x: str(datetime.datetime.strptime(x[1].split(\" \")[0], '%Y-%m-%d').strftime('%Y-%m')) == cur_date and x[0]== 'MobalyticsHQ', tweets))\n",
    "                    print(temp)\n",
    "\n",
    "                    neg = 0 # negative count\n",
    "                    pos = 0 # positive count\n",
    "                    for k in range(len(temp)):\n",
    "                        if temp[k][5] == 'NEGATIVE':\n",
    "                            neg += 1\n",
    "                        elif temp[k][5] == 'POSITIVE':\n",
    "                            pos += 1\n",
    "                    temp_df = pd.DataFrame([[ai_df.loc[i, 'permalink'], cur_date, neg, pos]], columns=['permalink', 'day', 'negative_count', 'positive_count'])\n",
    "                    print(temp_df) # keep track of progress\n",
    "                    ai_df_vis = ai_df_vis.append(temp_df, ignore_index=True)\n",
    "                except:\n",
    "                    print(\"error\", cur_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df_vis.day = pd.to_datetime(ai_df_vis['day'], format='%Y-%m')\n",
    "ai_df_vis.plot(x='day', y=['negative_count', 'positive_count'], figsize=(15,5), grid=True, color=['r', 'g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ai_df_vis.plot(x='day', y=['negative_count', 'positive_count'], figsize=(50,5), grid=True, x_compat=True)\n",
    "dates.datestr2num(date)\n",
    "# set monthly locator\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=3))\n",
    "# set formatter\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "# set font and rotation for date tick labels\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only Google news api only 6 months ago, very hard to get old news articles\n",
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()\n",
    "googlenews.search('yunxi-technology')\n",
    "googlenews.getpage(2)\n",
    "googlenews.result()\n",
    "ai_df['google_articles'] = np.empty((len(ai_df), 0)).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
